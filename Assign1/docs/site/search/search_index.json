{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Mining Big Data Assignment Documentation This Documentation consists of the answers to the assignment questions and the code used to solve the assignment. You can find the readme file in the particular exercise folders for more information. Private Repository till grades are released Due to academic integrity policies, The source code will be submitted in the group's tab. And all the information about the code & application can be found in the documentation and report submitted. Assignment 1: Basics and Map-Reduce (Group-75) Detailed Documentation to how to run the code, unit test, output files can be found https://beingmani.github.io/MBD-Assignment/assign1/ for the different OS we have used for the assignment. (Cloudera VM, MacOS with Docker.) Assignment 2: Advance Map-Reduce (Group-7) Detailed Documentation to how to run the code, output files can be found https://beingmani.github.io/MBD-Assignment/assign2/ we have used in the assignment. The instruction to run the code is provided in the Assignment 1 documentation.","title":"Home"},{"location":"#welcome-to-mining-big-data-assignment-documentation","text":"This Documentation consists of the answers to the assignment questions and the code used to solve the assignment. You can find the readme file in the particular exercise folders for more information. Private Repository till grades are released Due to academic integrity policies, The source code will be submitted in the group's tab. And all the information about the code & application can be found in the documentation and report submitted.","title":"Welcome to Mining Big Data Assignment Documentation"},{"location":"#assignment-1-basics-and-map-reduce-group-75","text":"Detailed Documentation to how to run the code, unit test, output files can be found https://beingmani.github.io/MBD-Assignment/assign1/ for the different OS we have used for the assignment. (Cloudera VM, MacOS with Docker.)","title":"Assignment 1: Basics and Map-Reduce (Group-75)"},{"location":"#assignment-2-advance-map-reduce-group-7","text":"Detailed Documentation to how to run the code, output files can be found https://beingmani.github.io/MBD-Assignment/assign2/ we have used in the assignment. The instruction to run the code is provided in the Assignment 1 documentation.","title":"Assignment 2: Advance Map-Reduce (Group-7)"},{"location":"assign1/","text":"Welcome to Mining Big Data Assignment 1 Documentation This Documentation consists of the answers to the assignment questions and the code used to solve the assignment. You can find the readme file in the particular exercise folders for more information. Private Repository till grades are released Due to academic integrity policies, The source code will be submitted in the group's tab. And all the information about the code & application can be found in the documentation and report submitted. You can find the code in the exercise3 , exercise4 folder under both MacOS & Cloudera folders for the particular exercise. The instructions to run the program is given [here](#how-to-run-the-program) Exercise 3 Run the given program in both standalone and pseudo-distributed mode and record the outputs. The program should be able to run in both modes and the outputs should be the same. Exercise 4 Part 1 Program to count the number of words with the specific number of letters uses a Mapper to convert the input to key-value pairs of <length, 1> and Reducer will reduce the key-value pairs to <SameLength, n> . The input string is sanitized against delimiter to get the proper words. Dataflow is as follows: Part 2 Program to count the number of words (unique) with the specific number of letters, we have decided to submit two solutions as both have their significant impact on the computation. Unit test cases are written with the help of mockito and mrunit library. ChainMapper is not supported by MrUnit ChainMapper is not supported by MrUnit, so the unit testing is done for the mapper and reducers. But not the MapReduce together. The alternative way could be running the job under a \"test\" prefixed name would be a good choice. a. ChainMapper Using ChainMapper , the program will first convert the input to key-value pairs of <word, 1> and then the second mapper will get the unique words and convert them to <word, 1> . The third mapper will get the length of the word and replace the key-value pairs as <WordLength, 1> . The Reducer will reduce the key-value pairs to <WordLength, n> . The computation is slower compared to the other solution. For the firstInputFile, it takes around ~8 ms to complete the computation. But the resource consumption is lower. (datanode, resourcemanager, yarn, nodemanager.) Dataflow is as follows: b. Two MapReduce Jobs Considering working with Two MapReduce Jobs instead of a single job, the program will first convert the input to key-value pairs of <word, 1> and reducer will get the count of the words. The second job will map the unique words and convert them to <word, 1> and reduce the key-value pairs to <WordLength, n> . Compared to the other solution, it is faster. For the firstInputFile, it takes around ~2 ms to complete the computation. But the resource consumption is higher. (datanode, resourcemanager, yarn, nodemanager.) Dataflow is as follows: How to run the program. For MacOS, you can find the installation guide here For Windows, you can find the installation guide here Folder Structure The repository consists of two different OS folders as both of team members used different setup for running the assignment problems, For Mac OS with M1 Chip we have used Docker and VM Cloudera was done by the Assignment Handout instructions. Exercise 3 & 4 and the report latex file will found on the root. Assign1 \u251c\u2500\u2500 README . md \u251c\u2500\u2500 package . json \u251c\u2500\u2500 . gitignore \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 docs \u2502 \u2514\u2500\u2500 mkdocs . yml \u251c\u2500\u2500 MacOS \u2502 \u251c\u2500\u2500 Exercise4 \u2502 \u2502 \u251c\u2500\u2500 src \u2502 \u2502 \u2502 \u251c\u2500\u2500 com . mbdassign . wordfreq \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 CustomUniqueWordCount . java # Custom Unique Word Count Using Chain Mapper \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 CustomWordCount . java \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 UniqueWordsTwoJobs . java # Custom Unique Words Using Two MapReduceJobs \u2502 \u2502 \u2502 \u251c\u2500\u2500 test . mbdassign . wordfreq # Unit tests with MRUnit Library \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 CustomUniqueWordCountMRUnit . java \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 CustomWordCountMRUnit . java \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 UniqueWordsTwoJobsMRUnit . java \u2502 \u2502 \u251c\u2500\u2500 input # input folders \u2502 \u2502 \u251c\u2500\u2500 output # output folders ( Artifacts ) \u2502 \u251c\u2500\u2500 Exercise3 \u2502 \u2502 \u251c\u2500\u2500 src \u2502 \u2502 \u2502 \u251c\u2500\u2500 com . basicsetup . hadoop \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 WordCount . java \u2502 \u2502 \u251c\u2500\u2500 input # input folders \u2502 \u2502 \u251c\u2500\u2500 output # output folders ( Artifacts ) \u251c\u2500\u2500 VM Cloudera \u2502 \u251c\u2500\u2500 Exercise 3 \u2502 \u2502 \u251c\u2500\u2500 \u2502 \u2502 \u2502 \u251c\u2500\u2500 WordCount \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 WordCount . java \u2502 \u2502 \u251c\u2500\u2500 input # input folders \u2502 \u2502 \u251c\u2500\u2500 output # output folders \u2502 \u2502 \u251c\u2500\u2500 Jars # SetupJars \u2502 \u251c\u2500\u2500 Exercise 4 \u2502 \u2502 \u251c\u2500\u2500 src \u2502 \u2502 \u2502 \u251c\u2500\u2500 FreqCounter \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 CustomUniqueWordCount . java # Custom Unique Word Count Using Chain Mapper \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 CustomWordCount . java \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 UniqueWordsTwoJobs . java # Custom Unique Words Using Two MapReduceJobs \u2502 \u2502 \u251c\u2500\u2500 input # input folders \u2502 \u2502 \u251c\u2500\u2500 output # output folders \u2502 \u2502 \u251c\u2500\u2500 Jars # SetupJars \u2514\u2500\u2500 Report . pdf Cloudera Instructions For the Cloudera Setup, we use VirtualBox. For the setting up the environment we have followed the steps given in the handout. Running the program The given source code is compiled and the jar file can be used to run the program. hdfs dfs -mkdir /input hdfs dfs -put input/* /input hdfs dfs -ls /input hdfs dfs -cat /input/input.txt hadoop jar WordCount.jar com.basicsetup.hadoop.WordCount /input/input.txt /output hdfs dfs -ls /output hdfs dfs -cat /output/part-r-00000 hdfs dfs -get /output MacOS Instructions Requirements The best way to install Docker on MacOS is Homebrew Cask. brew cask install docker # Install Docker open /Applications/Docker.app # Start Docker Using docker-hadoop To run Hadoop on Arm64 (Apple M1 Chip) and Intel Native we have used docker-hadoop Spinning up the environment for Standalone mode Open terminal and clone the docker-hadoop. Then run the following commands git clone https://github.com/wxw-matt/docker-hadoop # Clone the docker-hadoop cd hadoop-standalone # Go to the docker-hadoop folder docker build -t wxwmatt/hadoop-standalone:2.1.1-hadoop3.3.1-java8 . # Build the image Running the Hadoop Standalone mode Once the image is built, you can get in the container by running the following command docker container run --name standalone -it { $imageID } /bin/bash # Run the container To get the imageID You can use the command docker images to get the imageID. The commands given below will run the Hadoop Standalone mode. mkdir -p /hadoop/input/ # Create the input folder cp input/* /hadoop/input/ # Copy the input files to the input folder hadoop jar { $jarFileName } .jar { $className } /hadoop/input/ /hadoop/output/ # Run the jar file cat /hadoop/output/part-r-00000 # Print the output Spinning up the environment for Pseudo-distributed mode Open terminal and clone the docker-hadoop. Then run the following commands git clone https://github.com/wxw-matt/docker-hadoop # Clone the docker-hadoop cd docker-hadoop # Go to the docker-hadoop folder docker-compose up # Spin up the environment and wait for the environment to spin up. Running the Hadoop Job (Pseduo Distributed Mode) Once the environment is up and running, we can run the Hadoop Job. We can do the following inside the namenode container of the environment. The namenode container will be mounting the /app folder of the environment from the host machine. The /app folder will contain the jars and the input files for the program. docker exec -it namenode /bin/bash hdfs dfs -mkdir /input hdfs dfs -put /app/input/* /input hdfs dfs -ls /input hdfs dfs -cat /input/input.txt hadoop jar /app/WordCount.jar com.basicsetup.hadoop.WordCount /input/input.txt /output hdfs dfs -ls /output hdfs dfs -cat /output/part-r-00000 hdfs dfs -get /output exit The output of the program will be stored in the /output folder in the container, to get the output from the container, we can use the following command. docker cp namenode:/app/output . Your local machine will have the /output folder with the output of the program. Special Thanks for Matt \u2764\ufe0f for the work on Docker Hadoop .","title":"Assignment 1"},{"location":"assign1/#welcome-to-mining-big-data-assignment-1-documentation","text":"This Documentation consists of the answers to the assignment questions and the code used to solve the assignment. You can find the readme file in the particular exercise folders for more information. Private Repository till grades are released Due to academic integrity policies, The source code will be submitted in the group's tab. And all the information about the code & application can be found in the documentation and report submitted. You can find the code in the exercise3 , exercise4 folder under both MacOS & Cloudera folders for the particular exercise. The instructions to run the program is given [here](#how-to-run-the-program)","title":"Welcome to Mining Big Data Assignment 1 Documentation"},{"location":"assign1/#exercise-3","text":"Run the given program in both standalone and pseudo-distributed mode and record the outputs. The program should be able to run in both modes and the outputs should be the same.","title":"Exercise 3"},{"location":"assign1/#exercise-4","text":"","title":"Exercise 4"},{"location":"assign1/#part-1","text":"Program to count the number of words with the specific number of letters uses a Mapper to convert the input to key-value pairs of <length, 1> and Reducer will reduce the key-value pairs to <SameLength, n> . The input string is sanitized against delimiter to get the proper words. Dataflow is as follows:","title":"Part 1"},{"location":"assign1/#part-2","text":"Program to count the number of words (unique) with the specific number of letters, we have decided to submit two solutions as both have their significant impact on the computation. Unit test cases are written with the help of mockito and mrunit library. ChainMapper is not supported by MrUnit ChainMapper is not supported by MrUnit, so the unit testing is done for the mapper and reducers. But not the MapReduce together. The alternative way could be running the job under a \"test\" prefixed name would be a good choice.","title":"Part 2"},{"location":"assign1/#a-chainmapper","text":"Using ChainMapper , the program will first convert the input to key-value pairs of <word, 1> and then the second mapper will get the unique words and convert them to <word, 1> . The third mapper will get the length of the word and replace the key-value pairs as <WordLength, 1> . The Reducer will reduce the key-value pairs to <WordLength, n> . The computation is slower compared to the other solution. For the firstInputFile, it takes around ~8 ms to complete the computation. But the resource consumption is lower. (datanode, resourcemanager, yarn, nodemanager.) Dataflow is as follows:","title":"a. ChainMapper"},{"location":"assign1/#b-two-mapreduce-jobs","text":"Considering working with Two MapReduce Jobs instead of a single job, the program will first convert the input to key-value pairs of <word, 1> and reducer will get the count of the words. The second job will map the unique words and convert them to <word, 1> and reduce the key-value pairs to <WordLength, n> . Compared to the other solution, it is faster. For the firstInputFile, it takes around ~2 ms to complete the computation. But the resource consumption is higher. (datanode, resourcemanager, yarn, nodemanager.) Dataflow is as follows:","title":"b. Two MapReduce Jobs"},{"location":"assign1/#how-to-run-the-program","text":"For MacOS, you can find the installation guide here For Windows, you can find the installation guide here","title":"How to run the program."},{"location":"assign1/#folder-structure","text":"The repository consists of two different OS folders as both of team members used different setup for running the assignment problems, For Mac OS with M1 Chip we have used Docker and VM Cloudera was done by the Assignment Handout instructions. Exercise 3 & 4 and the report latex file will found on the root. Assign1 \u251c\u2500\u2500 README . md \u251c\u2500\u2500 package . json \u251c\u2500\u2500 . gitignore \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 docs \u2502 \u2514\u2500\u2500 mkdocs . yml \u251c\u2500\u2500 MacOS \u2502 \u251c\u2500\u2500 Exercise4 \u2502 \u2502 \u251c\u2500\u2500 src \u2502 \u2502 \u2502 \u251c\u2500\u2500 com . mbdassign . wordfreq \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 CustomUniqueWordCount . java # Custom Unique Word Count Using Chain Mapper \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 CustomWordCount . java \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 UniqueWordsTwoJobs . java # Custom Unique Words Using Two MapReduceJobs \u2502 \u2502 \u2502 \u251c\u2500\u2500 test . mbdassign . wordfreq # Unit tests with MRUnit Library \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 CustomUniqueWordCountMRUnit . java \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 CustomWordCountMRUnit . java \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 UniqueWordsTwoJobsMRUnit . java \u2502 \u2502 \u251c\u2500\u2500 input # input folders \u2502 \u2502 \u251c\u2500\u2500 output # output folders ( Artifacts ) \u2502 \u251c\u2500\u2500 Exercise3 \u2502 \u2502 \u251c\u2500\u2500 src \u2502 \u2502 \u2502 \u251c\u2500\u2500 com . basicsetup . hadoop \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 WordCount . java \u2502 \u2502 \u251c\u2500\u2500 input # input folders \u2502 \u2502 \u251c\u2500\u2500 output # output folders ( Artifacts ) \u251c\u2500\u2500 VM Cloudera \u2502 \u251c\u2500\u2500 Exercise 3 \u2502 \u2502 \u251c\u2500\u2500 \u2502 \u2502 \u2502 \u251c\u2500\u2500 WordCount \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 WordCount . java \u2502 \u2502 \u251c\u2500\u2500 input # input folders \u2502 \u2502 \u251c\u2500\u2500 output # output folders \u2502 \u2502 \u251c\u2500\u2500 Jars # SetupJars \u2502 \u251c\u2500\u2500 Exercise 4 \u2502 \u2502 \u251c\u2500\u2500 src \u2502 \u2502 \u2502 \u251c\u2500\u2500 FreqCounter \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 CustomUniqueWordCount . java # Custom Unique Word Count Using Chain Mapper \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 CustomWordCount . java \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 UniqueWordsTwoJobs . java # Custom Unique Words Using Two MapReduceJobs \u2502 \u2502 \u251c\u2500\u2500 input # input folders \u2502 \u2502 \u251c\u2500\u2500 output # output folders \u2502 \u2502 \u251c\u2500\u2500 Jars # SetupJars \u2514\u2500\u2500 Report . pdf","title":"Folder Structure"},{"location":"assign1/#cloudera-instructions","text":"For the Cloudera Setup, we use VirtualBox. For the setting up the environment we have followed the steps given in the handout.","title":"Cloudera Instructions"},{"location":"assign1/#running-the-program","text":"The given source code is compiled and the jar file can be used to run the program. hdfs dfs -mkdir /input hdfs dfs -put input/* /input hdfs dfs -ls /input hdfs dfs -cat /input/input.txt hadoop jar WordCount.jar com.basicsetup.hadoop.WordCount /input/input.txt /output hdfs dfs -ls /output hdfs dfs -cat /output/part-r-00000 hdfs dfs -get /output","title":"Running the program"},{"location":"assign1/#macos-instructions","text":"","title":"MacOS Instructions"},{"location":"assign1/#requirements","text":"The best way to install Docker on MacOS is Homebrew Cask. brew cask install docker # Install Docker open /Applications/Docker.app # Start Docker","title":"Requirements"},{"location":"assign1/#using-docker-hadoop","text":"To run Hadoop on Arm64 (Apple M1 Chip) and Intel Native we have used docker-hadoop","title":"Using docker-hadoop"},{"location":"assign1/#spinning-up-the-environment-for-standalone-mode","text":"Open terminal and clone the docker-hadoop. Then run the following commands git clone https://github.com/wxw-matt/docker-hadoop # Clone the docker-hadoop cd hadoop-standalone # Go to the docker-hadoop folder docker build -t wxwmatt/hadoop-standalone:2.1.1-hadoop3.3.1-java8 . # Build the image","title":"Spinning up the environment for Standalone mode"},{"location":"assign1/#running-the-hadoop-standalone-mode","text":"Once the image is built, you can get in the container by running the following command docker container run --name standalone -it { $imageID } /bin/bash # Run the container To get the imageID You can use the command docker images to get the imageID. The commands given below will run the Hadoop Standalone mode. mkdir -p /hadoop/input/ # Create the input folder cp input/* /hadoop/input/ # Copy the input files to the input folder hadoop jar { $jarFileName } .jar { $className } /hadoop/input/ /hadoop/output/ # Run the jar file cat /hadoop/output/part-r-00000 # Print the output","title":"Running the Hadoop Standalone mode"},{"location":"assign1/#spinning-up-the-environment-for-pseudo-distributed-mode","text":"Open terminal and clone the docker-hadoop. Then run the following commands git clone https://github.com/wxw-matt/docker-hadoop # Clone the docker-hadoop cd docker-hadoop # Go to the docker-hadoop folder docker-compose up # Spin up the environment and wait for the environment to spin up.","title":"Spinning up the environment for Pseudo-distributed mode"},{"location":"assign1/#running-the-hadoop-job-pseduo-distributed-mode","text":"Once the environment is up and running, we can run the Hadoop Job. We can do the following inside the namenode container of the environment. The namenode container will be mounting the /app folder of the environment from the host machine. The /app folder will contain the jars and the input files for the program. docker exec -it namenode /bin/bash hdfs dfs -mkdir /input hdfs dfs -put /app/input/* /input hdfs dfs -ls /input hdfs dfs -cat /input/input.txt hadoop jar /app/WordCount.jar com.basicsetup.hadoop.WordCount /input/input.txt /output hdfs dfs -ls /output hdfs dfs -cat /output/part-r-00000 hdfs dfs -get /output exit The output of the program will be stored in the /output folder in the container, to get the output from the container, we can use the following command. docker cp namenode:/app/output . Your local machine will have the /output folder with the output of the program. Special Thanks for Matt \u2764\ufe0f for the work on Docker Hadoop .","title":"Running the Hadoop Job (Pseduo Distributed Mode)"},{"location":"assign2/","text":"Welcome to Mining Big Data Assignment 2 Documentation This Documentation consists of the answers to the assignment 2 questions and the code used to solve the assignment. You can find the readme file in the particular exercise folders for more information. Private Repository till grades are released Due to academic integrity policies, The source code will be submitted in the group's tab. And all the information about the code & application can be found in the documentation and report submitted. You can find the code in the exercise1 , exercise3 folders for the particular exercise. The instructions to run the program is given in the Assignment 1. Please check out [here](https://beingmani.github.io/MBD-Assignment/assign1/) Exercise 3 With the help of a numpy library function linspace(start, end, num) the values of s are evenly distributed from 0 to 1. Calculating the S-Curve by substituting the values of s,r,b Exercise 4 Program to build an algorithm to recommend friends based on the number of mutual friends, we can say the pattern is given as User TAB Friends and for recommending the friend of friends. We will follow the below flowchart in the MapReduce approach, The Mapper will emit (user, user_friends) and all the possible tuples of friends of friends. Once we have the mapper values with these key,value pairs. In the reducer, we ignore the friends list as they are already friends with the user. Focus on the friends of friends and counting the mutual friends, sorting the list by count to get the top 10 recommendations for the user Folder Structure The repository has all the code for both assignment 1 and assignment 2, the folder structure for assignment 2 is as follows: Assign2 \u251c\u2500\u2500 README . md \u251c\u2500\u2500 . gitignore \u251c\u2500\u2500 Ex1 \u2502 \u251c\u2500\u2500 ex1 . ipynb \u2502 \u2514\u2500\u2500 ex1 . py \u2502 \u2514\u2500\u2500 s_curve . png \u251c\u2500\u2500 Ex3 \u2502 \u251c\u2500\u2500 input \u2502 \u251c\u2500\u2500 jars \u2502 \u2502 \u2514\u2500\u2500 friendrecommendation . jar \u2502 \u251c\u2500\u2500 src \u2502 \u2502 \u251c\u2500\u2500 com . mbdassign2 \u2502 \u2502 \u2502 \u251c\u2500\u2500 frdrecommender \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 FriendRecommendation . java \u2502 \u2502 \u251c\u2500\u2500 input # input folders \u2502 \u2502 \u251c\u2500\u2500 output # output folders \u2502 \u2502 \u251c\u2500\u2500 Jars # SetupJars \u2514\u2500\u2500 Report . pdf","title":"Assignment 2"},{"location":"assign2/#welcome-to-mining-big-data-assignment-2-documentation","text":"This Documentation consists of the answers to the assignment 2 questions and the code used to solve the assignment. You can find the readme file in the particular exercise folders for more information. Private Repository till grades are released Due to academic integrity policies, The source code will be submitted in the group's tab. And all the information about the code & application can be found in the documentation and report submitted. You can find the code in the exercise1 , exercise3 folders for the particular exercise. The instructions to run the program is given in the Assignment 1. Please check out [here](https://beingmani.github.io/MBD-Assignment/assign1/)","title":"Welcome to Mining Big Data Assignment 2 Documentation"},{"location":"assign2/#exercise-3","text":"With the help of a numpy library function linspace(start, end, num) the values of s are evenly distributed from 0 to 1. Calculating the S-Curve by substituting the values of s,r,b","title":"Exercise 3"},{"location":"assign2/#exercise-4","text":"Program to build an algorithm to recommend friends based on the number of mutual friends, we can say the pattern is given as User TAB Friends and for recommending the friend of friends. We will follow the below flowchart in the MapReduce approach, The Mapper will emit (user, user_friends) and all the possible tuples of friends of friends. Once we have the mapper values with these key,value pairs. In the reducer, we ignore the friends list as they are already friends with the user. Focus on the friends of friends and counting the mutual friends, sorting the list by count to get the top 10 recommendations for the user","title":"Exercise 4"},{"location":"assign2/#folder-structure","text":"The repository has all the code for both assignment 1 and assignment 2, the folder structure for assignment 2 is as follows: Assign2 \u251c\u2500\u2500 README . md \u251c\u2500\u2500 . gitignore \u251c\u2500\u2500 Ex1 \u2502 \u251c\u2500\u2500 ex1 . ipynb \u2502 \u2514\u2500\u2500 ex1 . py \u2502 \u2514\u2500\u2500 s_curve . png \u251c\u2500\u2500 Ex3 \u2502 \u251c\u2500\u2500 input \u2502 \u251c\u2500\u2500 jars \u2502 \u2502 \u2514\u2500\u2500 friendrecommendation . jar \u2502 \u251c\u2500\u2500 src \u2502 \u2502 \u251c\u2500\u2500 com . mbdassign2 \u2502 \u2502 \u2502 \u251c\u2500\u2500 frdrecommender \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 FriendRecommendation . java \u2502 \u2502 \u251c\u2500\u2500 input # input folders \u2502 \u2502 \u251c\u2500\u2500 output # output folders \u2502 \u2502 \u251c\u2500\u2500 Jars # SetupJars \u2514\u2500\u2500 Report . pdf","title":"Folder Structure"}]}